{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic skill suggestions using detailed description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project\n",
    "I decided to write a algorithm that can predict which skills are neccessary for a job post based on the description. As a result the working algorithm could suggest skills to add to a job after the employer has written the description of the job. \n",
    "\n",
    "### Data\n",
    "To do so, I have gathered 2000 job posts from the website using the API provided to me by Manuel Montes. From these 2000 posts I have saved the description and the required skills in separate arrays. The description will be used as input for my ML algorithms while the skills will function as output.\n",
    "\n",
    "### TF TFIDF\n",
    "After this I have calculated the TF and the TFIDF values. The Term Frequency is basically the word count over the entire database, the TFIDF value indicates how much a word is used in one job post relative to the entire database. So, if the entire database has the word \"Job\" in it 2000 times, then the word is less important one job post. \n",
    "\n",
    "### Data Pre-processing\n",
    "I've preprocessed the data (Removed null values, unknown values andd stopwords and reshaped the data). Also 50% of the jobs were in spanish. This shouldn't matter for the machine learning algorithms too much, but due to my lack of spanish I decided to remove them for better understanding of the algorithms output. Finally I splitted the data in train and test data 80/20\n",
    "\n",
    "### Training & Evaluation\n",
    "Finally I've trained 2 multiclass classification algorithms. KNN and Random forest. I tried tweeking them, but there are no metrics for multiclass predictions in the sklearn yet and due to the lack of time I didn't get to write them myself. This made optimization a lot harder. Also the 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of with gathering data from the server. Normally this would just be a download or I would run my code on the cloud, but since I only have access to an API. The only way to gather data is to do a lot of API calls\n",
    "\n",
    "Neccessary imports to do API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API call to get the index of 2000 job posts. These indexes are later used to get the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "url = f\"https://search.torre.co/opportunities/_search/?q=%20language%3AEnglish%29[offset={0}&size={2000}&aggregate={10}]\"\n",
    "query_response = session.post(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the index and save them in ids array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_responsejson = query_response.json()\n",
    "ids = []\n",
    "for element in query_responsejson['results']:\n",
    "    ids.append(element['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all data to idsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained indexes are used to do API calls to https://torre.co/api/opportunities/#IDOFJOBPOST the results are added to idsearch array and strengths array.\n",
    "\n",
    "#### I disabled this because the data is saved as npy values and the API calls are not neccessary anymore"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import datetime\n",
    "idsearch = []\n",
    "oldstrengths =[]\n",
    "for index, identifier in enumerate(ids):\n",
    "    if(index%10==0):\n",
    "        print('calls: '+str(index)+' time: '+str(datetime.datetime.now()))\n",
    "    strengths = []\n",
    "    result = requests.get(\"https://torre.co/api/opportunities/\"+identifier).json()\n",
    "    idsearch.append(result['details'])\n",
    "    for strength in result['strengths']:\n",
    "        strengths.append(strength['name'])\n",
    "    oldstrengths.append(strengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TFIDF values to summerize text on word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some neccessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "\n",
    "import re           \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from the files idsearch.npy and oldstrenghts.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as csv file\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "# define data\n",
    "# idsearchdata = np.array(idsearch)\n",
    "# oldstrengthsdata = np.array(oldstrengths)\n",
    "\n",
    "# save to csv file\n",
    "# np.save('idsearch.npy',idsearch)\n",
    "# np.save('oldstrengths.npy',oldstrengths)\n",
    "\n",
    "#load data from npy file\n",
    "idsearch = np.load('idsearch.npy', allow_pickle=True)\n",
    "oldstrengths = np.load('oldstrengths.npy', allow_pickle=True)\n",
    "# print(len(idsearch))\n",
    "# print(len(teststrengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run over all the data. We remove the spanish job post and we add the content of every job post to a separate array element. The strengths are saved in another array relative to their job posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "database=''\n",
    "test_database = ''\n",
    "databaseperoppertunity = []\n",
    "teststrengths =oldstrengths.copy()\n",
    "print(len(idsearch))\n",
    "print(len(teststrengths))\n",
    "removelater = []\n",
    "for index,oppertunity in enumerate(idsearch):\n",
    "    tempdata = ''\n",
    "    for textblock in oppertunity:\n",
    "        tempdata += textblock['content']\n",
    "    \n",
    "    if len(tempdata)>0:\n",
    "        if detect(tempdata)=='en':\n",
    "            databaseperoppertunity.append(tempdata)\n",
    "        else:\n",
    "            removelater.append(index)\n",
    "            teststrengths[index]='clean'\n",
    "    else:\n",
    "        removelater.append(index)\n",
    "        teststrengths[index]='clean'\n",
    "\n",
    "teststrengths  = np.delete(teststrengths, removelater)\n",
    "# for index  in reversed(removelater):\n",
    "#     if (teststrengths[index]=='clean'):\n",
    "#         teststrengths.pop(index)\n",
    "#     else:\n",
    "#         print(teststrengths[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "1161\n"
     ]
    }
   ],
   "source": [
    "print(len(databaseperoppertunity))\n",
    "print(len(teststrengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize all texts to sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = nltk.sent_tokenize(database)\n",
    "tokenized_word = nltk.word_tokenize(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words_spain= (set(stopwords.words(\"spanish\")))\n",
    "\n",
    "filtered_list = []\n",
    "for article in databaseperoppertunity:\n",
    "    tokens = word_tokenize(article) \n",
    "    filtered_article_list = [w for w in tokens if not w.lower() in stop_words and not w.lower() in stop_words_spain]\n",
    "    filtered_article = ''\n",
    "    for word in filtered_article_list:\n",
    "        if word[0].isalpha():\n",
    "            filtered_article+=' '+word\n",
    "    \n",
    "    filtered_list.append(filtered_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TF and TFIDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12072)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    "\n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(filtered_list)\n",
    "\n",
    "word_count_vector[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>experience</td>\n",
       "      <td>1.083419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>team</td>\n",
       "      <td>1.145155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>work</td>\n",
       "      <td>1.154151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>working</td>\n",
       "      <td>1.362099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>skills</td>\n",
       "      <td>1.482822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ged</td>\n",
       "      <td>7.364751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>geico</td>\n",
       "      <td>7.364751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gendered</td>\n",
       "      <td>7.364751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>redeeming</td>\n",
       "      <td>7.364751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>02</td>\n",
       "      <td>7.364751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12072 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            idf_weights\n",
       "experience     1.083419\n",
       "team           1.145155\n",
       "work           1.154151\n",
       "working        1.362099\n",
       "skills         1.482822\n",
       "...                 ...\n",
       "ged            7.364751\n",
       "geico          7.364751\n",
       "gendered       7.364751\n",
       "redeeming      7.364751\n",
       "02             7.364751\n",
       "\n",
       "[12072 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TFIDF values of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector=cv.transform(filtered_list)\n",
    "\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12072, 1)\n",
      "['React', 'Node.js', 'Fullstack Development', 'Javascript', 'TypeScript', 'HTML', 'CSS']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>clevertech</td>\n",
       "      <td>0.478741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>personal</td>\n",
       "      <td>0.141059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>put</td>\n",
       "      <td>0.119759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>currently</td>\n",
       "      <td>0.105156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>backgrounds</td>\n",
       "      <td>0.105156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feeds</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feel</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feelings</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feels</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>zuul</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12072 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tfidf\n",
       "clevertech   0.478741\n",
       "personal     0.141059\n",
       "put          0.119759\n",
       "currently    0.105156\n",
       "backgrounds  0.105156\n",
       "...               ...\n",
       "feeds        0.000000\n",
       "feel         0.000000\n",
       "feelings     0.000000\n",
       "feels        0.000000\n",
       "zuul         0.000000\n",
       "\n",
       "[12072 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "index = randint(0,10)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "    \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[index]\n",
    "\n",
    "print(first_document_vector.T.todense().shape)\n",
    "#print the scores\n",
    "print(teststrengths[index])\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "1161\n"
     ]
    }
   ],
   "source": [
    "print(len(teststrengths))\n",
    "print(len(databaseperoppertunity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1161, 12072)\n",
      "(1161, 1)\n"
     ]
    }
   ],
   "source": [
    "# classes = []\n",
    "# for skillList in teststrengths:\n",
    "#     for skill in skillList:\n",
    "#         if not(skill in classes):\n",
    "#             classes.append(skill)\n",
    "\n",
    "X = pd.DataFrame(tf_idf_vector.todense())\n",
    "Y = pd.DataFrame(teststrengths)\n",
    "Y = Y.astype('str')\n",
    "Y.replace(np.nan, '', regex=True)\n",
    "Y.fillna('', inplace=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X,  test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train, Y_test = train_test_split(Y,  test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(928, 12072)\n",
      "(233, 12072)\n",
      "(928, 1)\n",
      "(233, 1)\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(type(Y_train.values[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the networks are trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto',\n",
       "                                                     leaf_size=30,\n",
       "                                                     metric='minkowski',\n",
       "                                                     metric_params=None,\n",
       "                                                     n_jobs=None, n_neighbors=9,\n",
       "                                                     p=2, weights='uniform'),\n",
       "                      n_jobs=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "classifierKNN = MultiOutputClassifier(knn, n_jobs=1)\n",
    "classifierKNN.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomforrest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run out of memory here, restart the notebook and decrease the n_estimators size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "classifierRF=RandomForestClassifier(n_estimators=100)\n",
    "classifierRF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifierRF.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852     0.0\n",
       "1157    0.0\n",
       "450     0.0\n",
       "17      0.0\n",
       "971     0.0\n",
       "       ... \n",
       "127     0.0\n",
       "265     0.0\n",
       "512     0.0\n",
       "1107    0.0\n",
       "940     0.0\n",
       "Name: 0, Length: 233, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['hands-on development', 'Exemplary communication skills', 'Managing projects', 'Developer']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = randint(0,200)\n",
    "np.array(Y_test.iloc[[index]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Marketing', 'Excel']\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12072, 1)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# TEMPORARY GENERATES RANDOM INPUTS\n",
    "from random import randint\n",
    "index = randint(0,100)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "    \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[index]\n",
    "# first_document_vector= tf_idf_vector_test[0]\n",
    "\n",
    "print(first_document_vector.T.todense().shape)\n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12072)\n",
      "(233, 12072)\n",
      "['SCORM', 'AICCC', 'TinCan', 'xAPI', 'Javascript', 'Node.js', 'React', 'LMS', 'Communication skills']\n",
      "[\"['React', 'Javascript', 'PHP', 'HTML', 'CSS', 'WordPress', 'Articles', 'Tutorial']\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lms</td>\n",
       "      <td>0.350028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>articulate</td>\n",
       "      <td>0.255815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xapi</td>\n",
       "      <td>0.175014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>scorm</td>\n",
       "      <td>0.175014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aicc</td>\n",
       "      <td>0.175014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fedramp</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fedwire</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fee</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feed</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>zuul</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12072 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfidf\n",
       "lms         0.350028\n",
       "articulate  0.255815\n",
       "xapi        0.175014\n",
       "scorm       0.175014\n",
       "aicc        0.175014\n",
       "...              ...\n",
       "fedramp     0.000000\n",
       "fedwire     0.000000\n",
       "fee         0.000000\n",
       "feed        0.000000\n",
       "zuul        0.000000\n",
       "\n",
       "[12072 rows x 1 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.T.shape)\n",
    "print(X_test.shape)\n",
    "T_true = teststrengths[index]\n",
    "print(T_true)\n",
    "Y_pred = classifierRF.predict(df.T)\n",
    "print(Y_pred)\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'RFClassiferextrasmall.joblib.pkl'\n",
    "_ = joblib.dump(classifierRF, filename)\n",
    "filenametransformer = 'Transformer.joblib.pkl'\n",
    "_ = joblib.dump(tfidf_transformer, filenametransformer)\n",
    "\n",
    "filenamewordcount = 'word_count_vector.joblib.pkl'\n",
    "_ = joblib.dump(cv, filenamewordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': '{\"0\":{\"0\":\"[\\'Software Development\\', \\'React\\', \\'English\\']\"}}'}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://127.0.0.1:5000/'\n",
    "inputvariable = ['aws']\n",
    "\n",
    "params ={'query': inputvariable}\n",
    "response = requests.get(url, params)\n",
    "print(response)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
